{
    "docs": [
        {
            "location": "/", 
            "text": "Warcbase is an open-source platform for managing web archives built on Hadoop and HBase. The platform provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing via Spark.\n\n\nUse Warcbase to \nunleash your web archives\n! This wiki will show you how...\n\n\n\n\nNote: many of these tutorials currently assume a working knowledge of a Unix command line environment. For a conceptual and practical introduction, please see Ian Milligan and James Baker's \"Introduction to the Bash Command Line\" at the \nProgramming Historian\n.\n\n\n\n\nIf you've just arrived, you're probably interested in using \nSpark to analyze your web archive collections\n: gathering collection statistics, textual analysis, network analysis, etc.\n\n\nIf you want to explore web archives using other means, we have walkthroughs to use the SHINE front end on Solr indexes generated using Warcbase. See \nthis SHINE walkthrough\n and this \nbuilding Lucene indexes\n walkthrough.", 
            "title": "Summary"
        }, 
        {
            "location": "/Spark-Analysis-of-Site-Link-Structure/", 
            "text": "The following Spark script generates the aggregated site-level link structure, grouped by crawl date (YYYYMMDD):\n\n\nimport org.warcbase.spark.matchbox.{ExtractLinks, RecordLoader}\nimport org.warcbase.spark.rdd.RecordRDD._\n\nRecordLoader.loadArc(\n/path/to/arc\n, sc)\n  .keepValidPages()\n  .map(r =\n (r.getCrawldate, ExtractLinks(r.getUrl, r.getBodyContent)))\n  .flatMap(r =\n r._2.map(f =\n (r._1, f._1.replaceAll(\n^.*www\\\\.\n, \n), f._2.replaceAll(\n^.*www\\\\.\n, \n))))\n  .filter(r =\n r._2 != null \n r._3 != null)\n  .countItems()\n  .filter(r =\n r._2 \n 10)\n  .saveAsTextFile(\ncpp.sitelinks/\n)", 
            "title": "Analysis of Site Link Structure"
        }, 
        {
            "location": "/Spark-Collection-Analytics/", 
            "text": "You may want to learn what top-level domains you have in a given ARC, WARC, or directory of them. In the Spark Notebook, the following command will generate an interactive visualization.\n\n\nval r = \nRecordLoader.loadArc(arcdir, \nsc) \n.keepValidPages() \n.map(r =\n ExtractTopLevelDomain(r.getUrl)) \n.countItems() \n.take(10) \n\n\n\n\nIf you are using WARC files, change \nloadArc\n to \nloadWarc\n. If you want to see more than ten results, change the variable in the last line. \n\n\nHere is a sample output from a 5GB collection of Canadian political party ARCs:", 
            "title": "Collection Analytics"
        }, 
        {
            "location": "/Spark-Extracting-Domain-Level-Plain-Text/", 
            "text": "All Plain text\n\n\nThis script extracts the crawl date, domain, URL, and plain text from HTML files in the sample ARC data (and saves the output to out/). If you are using WARC files, change \nloadArc\n to \nloadWarc\n.\n\n\nimport org.warcbase.spark.matchbox.RecordLoader\nimport org.warcbase.spark.rdd.RecordRDD._\n\nRecordLoader.loadArc(\nsrc/test/resources/arc/example.arc.gz\n, sc)\n  .keepValidPages()\n  .map(r =\n (r.getCrawldate, r.getDomain, r.getUrl, r.getRawBodyContent))\n  .saveAsTextFile(\nout/\n)\n\n\n\n\nIf you wanted to use it on your own collection, you would change \"src/test/resources/arc/example.arc.gz\" to the directory with your own ARC or WARC files, and change \"out/\" on the last line to where you want to save your output data.\n\n\nNote that this will create a new directory to store the output, which cannot already exist.\n\n\nIf you want to run it in your Spark Notebook, the following script will show in-notebook plain text:\n\n\nval r = \nRecordLoader.loadWarc(\n/path/to/warcs\n,\nsc) \n.keepMimeTypes(Set(\ntext/html\n)) \n.discardDate(null) \n.map(r =\n { \nval t = ExtractRawText(r.getBodyContent) \nval len = 1000 \n(r.getCrawldate, r.getUrl, if ( t.length \n len ) t.substring(0, \nlen) else t)}) \n.collect() \n\n\n\n\nPlain text by URL Pattern\n\n\nThe following Spark script generates plain text renderings for all the web pages in a collection with a URL matching a filter string. In the example case, it will go through the collection and find all of the URLs within the \"greenparty.ca\" domain.\n\n\nimport org.warcbase.spark.matchbox.RecordLoader\nimport org.warcbase.spark.rdd.RecordRDD._\n\nRecordLoader.loadArc(\n/path/to/input\n, sc)\n  .keepValidPages()\n  .keepDomains(Set(\ngreenparty.ca\n))\n  .map(r =\n (r.getCrawldate, r.getDomain, r.getUrl, r.getRawBodyContent))\n  .saveAsTextFile(\n/path/to/output\n)", 
            "title": "Extracting Domain Level Plain Text"
        }, 
        {
            "location": "/Spark-Named-Entity-Recognition/", 
            "text": "The following Spark scripts use the \nStanford Named Entity Recognizer\n to extract names of entities \u2013 persons, organizations, and locations\u00a0\u2013 from collections of ARC/WARC files or extracted texts.\n\n\nThe scripts require a NER classifier model. There is one provided in the Stanford NER package (in the \nclassifiers\n folder) called \nenglish.all.3class.distsim.crf.ser.gz\n, but you can also use your own.\n\n\nExtract entities from ARC/WARC files\n\n\nimport org.warcbase.spark.matchbox.ExtractEntities\n\nsc.addFile(\n/path/to/classifier\n)\n\nExtractEntities.extractFromRecords(\nenglish.all.3class.distsim.crf.ser.gz\n, \n/path/to/arc/or/warc/files\n, \noutput/\n, sc)\n\n\n\n\nNote the call to \naddFile()\n. This is necessary if you are running this script on a cluster; it puts a copy of the classifier on each worker node. The classifier and input file paths may be local or on the cluster (e.g., \nhdfs:///user/joe/collection/\n).\n\n\nThe output of this script and the one below will consist of lines that look like this:\n\n\n(20090204,http://greenparty.ca/fr/node/6852?size=display,{\nPERSON\n:[\nParti Vert\n,\nPaul Maillet\n,\nAdam Saab\n],\n\nORGANIZATION\n:[\nGPC Candidate Ottawa Orleans\n,\nContact Cabinet\n,\nAccueil Paul Maillet GPC Candidate Ottawa Orleans Original\n,\nCirconscriptions Nouvelles \u00c9v\u00e9nements Blogues Politiques Contact Mon Compte\n],\n\nLOCATION\n:[\nCanada\n,\nCanada\n,\nCanada\n,\nCanada\n]})\n\n\n\n\nExtract entities from extracted text\n\n\nRun this script on texts \nalready extracted\n from an ARC/WARC collection.\n\n\nimport org.warcbase.spark.matchbox.ExtractEntities\n\nsc.addFile(\n/path/to/classifier\n)\n\nExtractEntities.extractFromScrapeText(\nenglish.all.3class.distsim.crf.ser.gz\n, \n/path/to/extracted/text\n, \noutput/\n)", 
            "title": "Named Entity Recognition"
        }, 
        {
            "location": "/Spark-Named-Entity-Recognition/#extract-entities-from-arcwarc-files", 
            "text": "import org.warcbase.spark.matchbox.ExtractEntities\n\nsc.addFile( /path/to/classifier )\n\nExtractEntities.extractFromRecords( english.all.3class.distsim.crf.ser.gz ,  /path/to/arc/or/warc/files ,  output/ , sc)  Note the call to  addFile() . This is necessary if you are running this script on a cluster; it puts a copy of the classifier on each worker node. The classifier and input file paths may be local or on the cluster (e.g.,  hdfs:///user/joe/collection/ ).  The output of this script and the one below will consist of lines that look like this:  (20090204,http://greenparty.ca/fr/node/6852?size=display,{ PERSON :[ Parti Vert , Paul Maillet , Adam Saab ], ORGANIZATION :[ GPC Candidate Ottawa Orleans , Contact Cabinet , Accueil Paul Maillet GPC Candidate Ottawa Orleans Original , Circonscriptions Nouvelles \u00c9v\u00e9nements Blogues Politiques Contact Mon Compte ], LOCATION :[ Canada , Canada , Canada , Canada ]})", 
            "title": "Extract entities from ARC/WARC files"
        }, 
        {
            "location": "/Spark-Named-Entity-Recognition/#extract-entities-from-extracted-text", 
            "text": "Run this script on texts  already extracted  from an ARC/WARC collection.  import org.warcbase.spark.matchbox.ExtractEntities\n\nsc.addFile( /path/to/classifier )\n\nExtractEntities.extractFromScrapeText( english.all.3class.distsim.crf.ser.gz ,  /path/to/extracted/text ,  output/ )", 
            "title": "Extract entities from extracted text"
        }, 
        {
            "location": "/Spark:-Installing-Spark-Notebook-on-a-Cloud-Computer/", 
            "text": "This is a walkthrough for installing Warcbase and Spark on a Ubuntu_14.04_Trusty x86_64 (QCOW2) image provided for Compute Canada. Presumably Amazon EC2 would provide a similar sort of experience. For more information on Warcbase, \ncheck out the repository here\n.\n\n\nIt is a bit bare boned as it assumes some knowledge of a command line environment.\n\n\nStep One: SSH to the server\n\n\n\n\nFor me on a Compute Canada instance, it's \nssh -i macpro.key ubuntu@IPADDRESS\n.\n\n\n\n\nStep Two: Install dependencies\n\n\n\n\nsudo apt-get update\n\n\nsudo apt-get install htop\n\n\nsudo apt-get install git\n\n\nsudo apt-get install maven\n\n\nsudo apt-get install scala\n\n\nexport JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\n\n\nsudo apt-get install openjdk-7-jdk\n\n\n\n\nStep Three: Set up the server properly\n\n\n\n\ntype \necho $HOSTNAME\n\n\ntry \nping $HOSTNAME\n: if it responds with something like \nping: unknown host milligan-wahr-05\n you need to add an entry to your \n/etc/hosts\n file\n\n\nsudo vim /etc/hosts\n\n\nreplace the \nlocalhost\n entry with your hostname - in my case, the first line now reads: \n127.0.0.1 milligan-wahr-05\n\n\nnow try to \nping $HOSTNAME\n: if you begin to see packet transmission/receipt information, you're golden.\n\n\n\n\nStep Four: Install Spark\n\n\n\n\ndownload spark 1.5.1 from \nhere\n. I used \nthis version for direct download\n.\n\n\nfor the lazier among us: \nwget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz\n\n\ntar -xvf spark-1.5.1-bin-hadoop2.6.tgz\n\n\nremove the tar file: \nrm spark-1.5.1-bin-hadoop2.6.tgz\n\n\n\n\n\n\n\n\nStep Five: Install Warcbase\n\n\n\n\nbring yourself back to your home directory: \ncd --\n\n\ndownload warcbase: \ngit clone https://github.com/lintool/warcbase.git\n\n\nchange to the warcbase directory: \ncd warcbase\n\n\nbuild warcbase: \nmvn clean package appassembler:assemble -DskipTests\n\n\n\n\nStep Six: Test that Warcbase and Spark are working\n\n\n\n\nverify that the shell works by navigating to your spark-shell directory and running: \n./bin/spark-shell\n\n\nif you don't get a bunch of errors, try: \n./bin/spark-shell --jars /home/ubuntu/warcbase/target/warcbase-0.1.0-SNAPSHOT-fatjar.jar\n to initalize warcbase\n\n\nTry this following script. In order to paste code, type \npaste\n and then Ctrl+D when you finish it up.\n\n\n\n\nval r = RecordLoader.loadArc(\n/home/ubuntu/warcbase/src/test/resources/arc/example.arc.gz\n, sc)\n  .keepValidPages()\n  .map(r =\n ExtractTopLevelDomain(r.getUrl))\n  .countItems()\n  .take(10)\n\n\n\n\nIf you receive the following output:\n\n\nr: Array[(String, Int)] = Array((www.archive.org,132), (deadlists.com,2), (www.hideout.com.br,1))\n\n\n\n\nThen you're working.\n\n\nStep Seven: Getting the Spark Notebook working\n\n\n\n\ndownload it with this command: \nwget https://s3.eu-central-1.amazonaws.com/spark-notebook/tgz/spark-notebook-master-scala-2.10.4-spark-1.5.1-hadoop-2.6.0-cdh5.4.2.tgz\n\n\nunzip it: \ntar -xvf spark-notebook-master-scala-2.10.4-spark-1.5.1-hadoop-2.6.0-cdh5.4.2.tgz\n\n\ntest that it works: \n./bin/spark-notebook\n\n\n\n\nThe catch is that you'll want to view it on a browser, but you're working on a server. \n\n\nStep Eight: Deployment\n\n\nWhile it is easy to deploy, at least for quick testing purposes, with \nsudo ./bin/spark-notebook -Dhttp.port=80\n, this will leave your Spark Notebook wide open to the world. As you've got read/write privileges, unless you really don't care about your machine or your data, this isn't the best way.\n\n\nInstead, open an SSH tunnel to your instance. You'll need to reconnect using \nssh\n. The following command should establish a tunnel from the remote localhost:9000 to your local localhost:9000.\n\n\nssh -i macpro.key ubuntu@MYIPADDRESS -L 9000:127.0.0.1:9000\n\n\nOnce in, deploy your Spark Notebook by running\n\n\nsudo ./bin/spark-notebook -Dhttp.port=9000\n from your spark-notebook directory (in my case, that is \n~/spark-notebook-0.6.2-SNAPSHOT-scala-2.10.4-spark-1.5.1-hadoop-2.6.0-cdh5.4.2\n).\n\n\nOn your local browser, point it to \nlocalhost:9000\n. You should see your spark-notebook!\n\n\n\n\nStep Nine: Tweaking\n\n\nYou might find that your jobs are taking too long. This might because you don't have enough executors set. \n\n\nFind how many CPU cores you have free by running \nhtop\n. On a lightweight cloud machine, you might see:\n\n\n\n\nThis shows four cores in action. So when you run your \nspark-shell\n as in Step Six above, you might want to pass \n--num-executors 4\n. Tweak and refine as needed.\n\n\nStep Ten: Have Fun\n\n\nYou've now got warcbase running in the cloud. What more could a person want?", 
            "title": "Installing Spark Notebook on a Cloud Computer"
        }, 
        {
            "location": "/Spark:-NER-Visualization/", 
            "text": "The Warcbase Spark \nmatchbox\n functions in \nExtractEntities\n list the named entities contained in each page in an archive, but we are often interested in getting a sense of what is contained in a collection as a whole. Visualization can help. We have provided a Javascript visualizer using \nD3.js\n that produces views of NER data. You can try the visualizer \nhere\n.\n\n\nThe visualizer can currently produce the following:\n\n a list view, with frequency of the selected entity type represented by font size (inspired by the \nTrading Consequences Location Cloud\n \n\n\n\n a standard word cloud for the selected entity type \n\n* a bubble chart, representing all entity types at once.\n\n\nGenerating NER Data\n\n\nThe \nmatchbox\n contains a function in \nNERCombinedJson\n that will extract NER entities from \nplain text records\n, summarize them by crawl date, and save the results as a single JSON file. The following script calls the function. Modify the file names in (1) and (2) as appropriate.\n\n\nimport org.warcbase.spark.matchbox.NERCombinedJson\n\nsc.addFile(\n/path/to/english.all.3class.distsim.crf.ser.gz\n) // (1)\n\nval ner = new NERCombinedJson\n\nner.classify(\nenglish.all.3class.distsim.crf.ser.gz\n, \nhdfs:///path/to/plaintext/\n, \nresults.json\n, sc) // (2)\n\n\n\n\nSetting Up the Visualizer\n\n\nTo use the visualizer with your own data you must place the files from \nwarcbase/vis/ner\n into a folder on a web server. If you wish to serve files locally the Python SimpleHTTPServer is \neasy to use\n. Because of cross-domain restrictions your web browser will only allow the visualizer to load JSON files from the same server, so place the data file somewhere accessible (local) to the web server.", 
            "title": "NER Visualization"
        }, 
        {
            "location": "/Spark:-NER-Visualization/#generating-ner-data", 
            "text": "The  matchbox  contains a function in  NERCombinedJson  that will extract NER entities from  plain text records , summarize them by crawl date, and save the results as a single JSON file. The following script calls the function. Modify the file names in (1) and (2) as appropriate.  import org.warcbase.spark.matchbox.NERCombinedJson\n\nsc.addFile( /path/to/english.all.3class.distsim.crf.ser.gz ) // (1)\n\nval ner = new NERCombinedJson\n\nner.classify( english.all.3class.distsim.crf.ser.gz ,  hdfs:///path/to/plaintext/ ,  results.json , sc) // (2)", 
            "title": "Generating NER Data"
        }, 
        {
            "location": "/Spark:-NER-Visualization/#setting-up-the-visualizer", 
            "text": "To use the visualizer with your own data you must place the files from  warcbase/vis/ner  into a folder on a web server. If you wish to serve files locally the Python SimpleHTTPServer is  easy to use . Because of cross-domain restrictions your web browser will only allow the visualizer to load JSON files from the same server, so place the data file somewhere accessible (local) to the web server.", 
            "title": "Setting Up the Visualizer"
        }, 
        {
            "location": "/Web-Archives-2015:-Playing-with-Spark/", 
            "text": "Sample data available here\n\n\nSee also this walkthrough for \nextracting plain text\n - written for the Spark shell.\n\n\nInitial command:\n\n\n:cp /Users/ianmilligan1/dropbox/warcbase/target/warcbase-0.1.0-SNAPSHOT-fatjar.jar\n\n\n\n\nCommand one:\n\n\nimport org.warcbase.spark.matchbox._ \nimport org.warcbase.spark.rdd.RecordRDD._ \n\n\n\n\nCommand two:\n\n\nval r = \nRecordLoader.loadArc(\n/Users/ianmilligan1/Dropbox/warcs-workshop/227-20051004191331-00000-crawling015.archive.org.arc.gz\n, \nsc) \n.keepValidPages() \n.map(r =\n ExtractTopLevelDomain(r.getUrl)) \n.countItems() \n.take(10) \n\n\n\n\nCommand three:\n\n\nval r = \nRecordLoader.loadArc(\n/Users/ianmilligan1/Dropbox/warcs-workshop/227-20051007202637-00000-crawling018.arc.gz\n,\nsc) \n.keepMimeTypes(Set(\ntext/html\n)) \n.discardDate(null) \n.map(r =\n { \nval t = ExtractRawText(r.getBodyContent) \nval len = 1000 \n(r.getCrawldate, r.getUrl, if ( t.length \n len ) t.substring(0, \nlen) else t)}) \n.collect() \n\n\n\n\nCommand four:\n\n\nimport org.warcbase.spark.matchbox.{ExtractLinks, RecordLoader}\nimport org.warcbase.spark.rdd.RecordRDD._\n\nRecordLoader.loadArc(\n/path/to/arc\n, sc)\n  .keepValidPages()\n  .map(r =\n (r.getCrawldate, ExtractLinks(r.getUrl, r.getBodyContent)))\n  .flatMap(r =\n r._2.map(f =\n (r._1, f._1.replaceAll(\n^.*www\\\\.\n, \n), f._2.replaceAll(\n^.*www\\\\.\n, \n))))\n  .filter(r =\n r._2 != null \n r._3 != null)\n  .countItems()\n  .filter(r =\n r._2 \n 10)\n  .saveAsTextFile(\ncpp.sitelinks/\n)", 
            "title": "Web Archives 2015: Playing with Spark"
        }, 
        {
            "location": "/Building-and-Running-Warcbase-Under-OS-X/", 
            "text": "Warcbase is a web archive \nplatform\n, not a single program. Its capabilities comprise two main categories:\n\n\n\n\nAnalysis of web archives using the \nPig\n programming language, and assorted helper scripts and utilities\n\n\nWeb archive database management, with support for the \nHBase\n distributed data store, and \nOpenWayback\n integration providing a friendly web interface to view stored websites\n\n\n\n\nOne can take advantage of the analysis tools (1) without bothering with the database management aspect of Warcbase -- in fact, most digital humanities researchers will probably find the former more useful. Users who are only interested in the analysis tools need only be concerned with the first two sections of this document (Prerequisites and Building Warcbase).\n\n\n(This document was written because installing Warcbase under OS X requires a number of minor changes to the \nofficial project instructions\n.)\n\n\nPrerequisites\n\n\n\n\nOS X Developer Tools\n\n\nHomebrew\n\n\nMaven (\nbrew install maven\n)\n\n\nHadoop (\nbrew install hadoop\n)\n\n\n\n\nHBase (\nbrew install hbase\n)\n\n\nConfigure HBase by making the changes to the following files located in the HBase installation directory, which will be something like \n/usr/local/Cellar/hbase/0.98.6.1/libexec/\n (depending on the version number).\n\n\n\n\nconf/hbase-site.xml\n:\nInsert within the \n tags\n\n\n    \nhbase.rootdir\n\n    \nfile:///Users/yourname/hbase\n\n\n\n\n\n    \nhbase.zookeeper.property.dataDir\n\n    \n/Users/yourname/zookeeper\n\n\n\n\n\n\nWhere \nyourname\n is your username. Feel free to choose other directories to store these files, used by HBase and its ZooKeeper instance, if you like. \nHBase will create these directories. If they already exist, they will cause problems later on.\n\n\n\n\nconf/hbase-env.sh\n: Look for the following line,\n      export HBASE_OPTS=\"-XX:+UseConcMarkSweepGC\"\n\n\n\n\nand change it to:\n\n\n  export HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -Djava.security.krb5.realm=-Djava.security.krb5.kdc=\"\n\n\n\nVerify that HBase is installed correctly by running the HBase shell:\n\n\n$ hbase shell\n\nhbase(main):001:0\n list\n\n// Some lines of log messages\n\n0 row(s) in 1.3060 seconds\n\n=\n []\n\nhbase(main):002:0\n exit\n\n\n\n\n\n\n\nTomcat (\nbrew install tomcat\n) \n(Only necessary for OpenWayback integration; skip otherwise)\n \n\n\n\n\n\n\nPig (\nbrew install pig\n)\n\n\n\n\n\n\n\n\nN.B.\n If you run an automatic Homebrew system update (\nbrew update \n brew upgrade\n) it is possible a new version of Hadoop, HBase, or Tomcat will be installed. The previous version will remain on your system, but the symbolic links in \n/ur/local/bin/\n will point to the new version; i.e., it is the new version that will be executed when you run any of the software's components, unless you specify the full pathname. There are two solutions:\n1. Re-configure the new version of the updated software according to the instructions above.\n2. Make the symbolic links point to the older version of the updated software, with the command \nbrew switch \nformula\n \nversion\n. E.g., \nbrew switch hbase 0.98.6.1\n.\n\n\n\n\nBuilding Warcbase\n\n\nTo start, you will need to clone the Warcbase Git repository:\n\n\n$ git clone http://github.com/lintool/warcbase.git\n\n\n\nFrom inside the root directory \nwarcbase\n, build the project:\n\n\n$ mvn clean package appassembler:assemble -DskipTests\n\n\n\nIf you leave off \n-DskipTests\n, the build may fail when it runs tests due to a shortage of memory. If you try the build with the tests and this happens, don't worry about it. \n\n\nBecause OS X is not quite case sensitive (it does not allow two files or directories spelled the same but for case), you must remove one file from the JAR package:\n\n\n$ zip -d target/warcbase-0.1.0-SNAPSHOT-fatjar.jar META-INF/LICENSE\n\n\n\nIngesting content\n\n\nTo ingest a directory of web archive files (which may be GZip-compressed, e.g., webcollection.arc.gz), run the following from inside the \nwarcbase\n directory. \n\n\n$ start-hbase.sh\n$ export CLASSPATH_PREFIX=\"/usr/local/Cellar/hbase/0.98.6.1/libexec/conf/\"\n$ sh target/appassembler/bin/IngestFiles -dir /path/to/webarchive/files/ -name archive_name -create -gz\n\n\n\nChange as appropriate the HBase configuration path (version number), the directory of web archive files, and the archive name. Use the option \n-append\n instead of \n-create\n to add to an existing database table. Note the \n-gz\n flag: this changes compression method to Gzip from the default Snappy, which is unavailable as a native Hadoop library on OS X. (The above commands assume you are using a shell in the \nbash\n family.)\n\n\nTip\n: To avoid repeatedly setting the CLASSPATH_PREFIX variable, add the \nexport\n line to your \n~/.bash_profile\n file.\n\n\nIf you wish to shut down HBase, the command is \nstop-hbase.sh\n. You can check if HBase is running with the command \njps\n; if it is running you will see the process \nHMaster\n listed. You can also view detailed server status information at http://localhost:60010/.\n\n\nRun and test the WarcBrowser\n\n\nYou may now view your archived websites through the WarcBrowser interface.\n\n\n# Start HBase first, if it isn't already running:\n$ start-hbase.sh\n# Set CLASSPATH_PREFIX, if it hasn't been done this terminal session:\n$ export CLASSPATH_PREFIX=\"/usr/local/Cellar/hbase/0.98.6.1/libexec/conf/\"\n# Start the browser:\n$ sh target/appassembler/bin/WarcBrowser -port 8079\n\n\n\nYou can now use \nhttp://localhost:8079/\n to browse the archive. For example:\n\n \nhttp://localhost:8079/archive_name/*/http://mysite.com/\n will give you a list of available versions of \nhttp://mysite.com/\n.\n\n \nhttp://localhost:8079/archive_name/19991231235959/http://mysite.com/\n will give you the record of \nhttp://mysite.com/\n just before Y2K.\n\n\nOpenWayback Integration\n\n\nFor a more functional, visually-appealing interface, you may install a custom version of the Wayback Machine. \n\n\nAssuming Tomcat is installed, start it by running:\n\n\n$ catalina start\n\n\n\nInstall OpenWayback by downloading the latest binary release \nhere\n. Extract the .tar.gz; inside it there will be a web application file \nopenwayback-(version).war\n. Copy this file into the \nwebapps\n folder of Tomcat, something like \n/usr/local/Cellar/tomcat/8.0.17/libexec/webapps/\n, and rename it \nROOT.war\n. Tomcat will immediately unpack this file into the \nROOT\n directory.\n\n\nIf you are running a current version of Tomcat (i.e., version 8) in combination with OpenWayback 2.0.0, edit the file \nwebapps/ROOT/WEB-INF/web.xml\n and insert a slash (\"/\") in front of the paths of parameters \nlogging-config-path\n and \nconfig-path\n. (\nDetails\n) Future releases of OpenWayback should already include this configuration change.\n\n\nAdd the Warcbase jar file to the Wayback installation, by copying \ntarget/appassembler/repo/org/warcbase/warcbase/0.1.0-SNAPSHOT/warcbase-0.1.0-SNAPSHOT.jar\n from the Warcbase build directory into Tomcat's \nwebapps/ROOT/WEB-INF/lib/\n. \n\n\nInto \nwebapps/ROOT/WEB-INF\n copy Warcbase's \nsrc/main/resources/BDBCollection.xml\n. In \nBDBCollection.xml\n, replace \nHOST\n, \nPORT\n, and \nTABLE\n with \nlocalhost\n, \n8079\n, and \narchive_name\n (or whatever the archive table in HBase is called).\n\n\nRestart Tomcat:\n\n\n$ catalina stop\n$ catalina start\n\n\n\nNow, navigate to http://localhost:8080/wayback/ and access one of your archived web pages through the Wayback interface.\n\n\nAnalytics\n\n\nWarcbase is useful for managing web archives, but its real power is as a platform for processing and analyzing the archives in its database. Its analysis tools are still under development, but at the moment you can use the tools described below, including the \npig\n scripting interface.\n\n\nBuilding the URL mapping\n\n\nMost of the tools that follow require a URL mapping file, which maps every URL in a set of ARC/WARC files to a unique integer ID. There are two ways of doing this; the first is simpler:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.UrlMappingMapReduceBuilder -input /path/to/webarc/files -output fst.dat\n\n\n\nIf this does not work due to a lack of memory, try the following steps:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /path/to/webarchive/files -output urls\n\n\n\n(If you have configured HBase to run in distributed mode rather than in standalone mode, which is the configuration provided above, you must now copy the \nurls\n directory out of HDFS into the local filesystem.)\n\n\nNext:\n\n\n$ sh target/appassembler/bin/UrlMappingBuilder -input /path/to/urls -output fst.dat\n\n\n\nWe can examine the FST data with the following utility program:\n\n\n# Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/\n\n\n\n(If you are running in distributed mode, now copy the \nfst.dat\n file into the HDFS, so it is accessible to the cluster:\n\n\n$ hadoop fs -put fst.dat /hdfs/path/\n\n\n\n)\n\n\n\n\nYou might have noticed, we are working here with ARC/WARC files and not tables in HBase. The same is done below as well. This is because most of the tools described her and below do not yet have HBase support.\n\n\n\n\nExtracting the webgraph\n\n\nWe can use the mapping data from above to extract the webgraph, with a Hadoop program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.wa\nrcbase.analysis.graph.ExtractLinksWac -hdfs /path/to/webarchive/files -output output -urlMapping fst.dat\n\n\n\nThe \n-hdfs\n flag is misleading; if HBase is running in standalone mode, this flag should specifiy a local path. \n\n\nThe resulting webgraph will appear in the \noutput\n directory, in one or more files with names like \npart-m-00000\n, \npart-m-00001\n, etc.\n\n\nExtracting a site-level webgraph\n\n\nInstead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, the following input files are needed:\n\n a CSV prefix file providing URL prefixes for each supernode (comma-delimited: ID, URL prefix). The first line of this file is ignored (reserved for headers). The URL prefix is a simple string representing a site, e.g., \nhttp://cnn.com/\n. The ID should be unique (I think), so take not of the total number of unique URLs extracted when building the URL mapping above, and make sure your IDs are larger than this.\n\n an FST mapping file to map individual URLs to unique integer ids (from above)\n\n\nThen run this MapReduce program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.ExtractSiteLinks -hdfs /path/to/webarchive/files -output output -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv\n\n\n\nOther analysis tools\n\n\nThe tools described in this section are relatively simple. Some can process ARC and/or WARC files, while others exist in ARC and WARC versions.\n\n\nThere are three counting tools. They count different content-types, crawl-dates, and urls:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcContentTypes -input /arc/files/ -output contentTypes\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcCrawlDates -input /arc/files/ -output crawlDates\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcUrls -input ~/arc/files/ -output urls\n\n\n\nFor WARC files, replace \"Arc\" with \"Warc\" in the class names (e.g., \norg.warcbase.analysis.CountArcContentTypes\n becomes \norg.warcbase.analysis.CountWarcContentTypes\n).\n\n\nThere is a tool to extract unique URLs:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /arc/or/warc/files -output uniqueUrls\n\n\n\nThere is a pair of tools to find URLs, according to a regex pattern. \n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindArcUrls -input /arc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindWarcUrls -input /warc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n\n\nThere is a tool to detect duplicates in the HBase:\n\n\n$ sh target/appassembler/bin/DetectDuplicates -name table\n\n\n\nThere is also a web graph tool that pulls out link anchor text (background: https://github.com/lintool/warcbase/issues/8). \n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.graph.InvertAnchorText -hdfs /arc/or/warc/files -output output -numReducers 1 -urlMapping fst.dat\n\n\n\nFor all of these tools that employ a URL mapping, you must use the FST mapping generated for the set of data files you are analyzing.\n\n\nPig integration\n\n\nWarcbase comes with Pig integration for manipulating web archive data. Pig scripts may be run from the interactive Grunt shell (run \npig\n), or, more conveniently, from a file (e.g., \npig -f extractlinks.pig\n).\n\n\nThe following script extracts links:\n\n\nregister 'target/warcbase-0.1.0-SNAPSHOT-fatjar.jar';\n\nDEFINE ArcLoader org.warcbase.pig.ArcLoader();\nDEFINE ExtractLinks org.warcbase.pig.piggybank.ExtractLinks();\n\nraw = load '/path/to/arc/files' using ArcLoader as\n  (url: chararray, date: chararray, mime: chararray, content: bytearray);\n\na = filter raw by mime == 'text/html';\nb = foreach a generate url, FLATTEN(ExtractLinks((chararray) content));\n\nstore b into '/output/path/';\n\n\n\n\nIn the output directory you should find data output files with source URL, target URL, and anchor text.", 
            "title": "Building and Running Warcbase Under OS X"
        }, 
        {
            "location": "/Building-and-Running-Warcbase-Under-OS-X/#analytics", 
            "text": "Warcbase is useful for managing web archives, but its real power is as a platform for processing and analyzing the archives in its database. Its analysis tools are still under development, but at the moment you can use the tools described below, including the  pig  scripting interface.  Building the URL mapping  Most of the tools that follow require a URL mapping file, which maps every URL in a set of ARC/WARC files to a unique integer ID. There are two ways of doing this; the first is simpler:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.UrlMappingMapReduceBuilder -input /path/to/webarc/files -output fst.dat  If this does not work due to a lack of memory, try the following steps:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /path/to/webarchive/files -output urls  (If you have configured HBase to run in distributed mode rather than in standalone mode, which is the configuration provided above, you must now copy the  urls  directory out of HDFS into the local filesystem.)  Next:  $ sh target/appassembler/bin/UrlMappingBuilder -input /path/to/urls -output fst.dat  We can examine the FST data with the following utility program:  # Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/  (If you are running in distributed mode, now copy the  fst.dat  file into the HDFS, so it is accessible to the cluster:  $ hadoop fs -put fst.dat /hdfs/path/  )   You might have noticed, we are working here with ARC/WARC files and not tables in HBase. The same is done below as well. This is because most of the tools described her and below do not yet have HBase support.   Extracting the webgraph  We can use the mapping data from above to extract the webgraph, with a Hadoop program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.wa\nrcbase.analysis.graph.ExtractLinksWac -hdfs /path/to/webarchive/files -output output -urlMapping fst.dat  The  -hdfs  flag is misleading; if HBase is running in standalone mode, this flag should specifiy a local path.   The resulting webgraph will appear in the  output  directory, in one or more files with names like  part-m-00000 ,  part-m-00001 , etc.  Extracting a site-level webgraph  Instead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, the following input files are needed:  a CSV prefix file providing URL prefixes for each supernode (comma-delimited: ID, URL prefix). The first line of this file is ignored (reserved for headers). The URL prefix is a simple string representing a site, e.g.,  http://cnn.com/ . The ID should be unique (I think), so take not of the total number of unique URLs extracted when building the URL mapping above, and make sure your IDs are larger than this.  an FST mapping file to map individual URLs to unique integer ids (from above)  Then run this MapReduce program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.ExtractSiteLinks -hdfs /path/to/webarchive/files -output output -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv  Other analysis tools  The tools described in this section are relatively simple. Some can process ARC and/or WARC files, while others exist in ARC and WARC versions.  There are three counting tools. They count different content-types, crawl-dates, and urls:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcContentTypes -input /arc/files/ -output contentTypes\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcCrawlDates -input /arc/files/ -output crawlDates\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcUrls -input ~/arc/files/ -output urls  For WARC files, replace \"Arc\" with \"Warc\" in the class names (e.g.,  org.warcbase.analysis.CountArcContentTypes  becomes  org.warcbase.analysis.CountWarcContentTypes ).  There is a tool to extract unique URLs:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /arc/or/warc/files -output uniqueUrls  There is a pair of tools to find URLs, according to a regex pattern.   $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindArcUrls -input /arc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindWarcUrls -input /warc/files/ -output foundUrls -pattern \"http://.*org/.*\"  There is a tool to detect duplicates in the HBase:  $ sh target/appassembler/bin/DetectDuplicates -name table  There is also a web graph tool that pulls out link anchor text (background: https://github.com/lintool/warcbase/issues/8).   $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.graph.InvertAnchorText -hdfs /arc/or/warc/files -output output -numReducers 1 -urlMapping fst.dat  For all of these tools that employ a URL mapping, you must use the FST mapping generated for the set of data files you are analyzing.  Pig integration  Warcbase comes with Pig integration for manipulating web archive data. Pig scripts may be run from the interactive Grunt shell (run  pig ), or, more conveniently, from a file (e.g.,  pig -f extractlinks.pig ).  The following script extracts links:  register 'target/warcbase-0.1.0-SNAPSHOT-fatjar.jar';\n\nDEFINE ArcLoader org.warcbase.pig.ArcLoader();\nDEFINE ExtractLinks org.warcbase.pig.piggybank.ExtractLinks();\n\nraw = load '/path/to/arc/files' using ArcLoader as\n  (url: chararray, date: chararray, mime: chararray, content: bytearray);\n\na = filter raw by mime == 'text/html';\nb = foreach a generate url, FLATTEN(ExtractLinks((chararray) content));\n\nstore b into '/output/path/';  In the output directory you should find data output files with source URL, target URL, and anchor text.", 
            "title": "Analytics"
        }, 
        {
            "location": "/Ingesting-Content-into-HBase/", 
            "text": "You can find some sample data \nhere\n. Ingesting data into Warcbase is fairly straightforward:\n\n\n$ setenv CLASSPATH_PREFIX \n/etc/hbase/conf/\n\n$ sh target/appassembler/bin/IngestFiles \\\n    -dir /path/to/warc/dir/ -name archive_name -create\n\n\n\n\nCommand-line options:\n\n\n\n\nUse the \n-dir\n option to specify the directory containing the data files.\n\n\nUse the \n-name\n option to specify the name of the archive (will correspond to the HBase table name).\n\n\nUse the \n-create\n option to create a new table (and drop the existing table if a table with the same name exists already). Alternatively, use \n-append\n to add to an existing table.\n\n\n\n\nAn example on one OS X machine:\n\n\n$ export CLASSPATH_PREFIX=\n/usr/local/Cellar/hbase/0.98.6.1/libexec/conf/\n\n$ sh target/appassembler/bin/IngestFiles -dir ~/desktop/WARC-directory/ -name webarchives1 -create -gz\n\n\n\n\nThat should do it. The data should now be in Warcbase.", 
            "title": "Ingesting Content into HBase"
        }, 
        {
            "location": "/Installing-and-Running-Spark-under-OS-X/", 
            "text": "Spark integration\n\n\nWarcbase comes with Spark integration for manipulating web archive data. \n\n\nPrerequisites\n\n\n\n\nScala (\nbrew install scala\n)\n\n\nSpark (\nbrew install apache-spark\n)\n\n\n\n\nFluent API\n\n\nA fluent API is being developed for RDDs, and is now available on master.\n\n\nTo use the API, two imports are required (run within Spark shell):    \n\n\nimport org.warcbase.spark.rdd.RecordRDD._\nimport org.warcbase.spark.matchbox.RecordLoader\n\n\n\n\nRunning in the Spark Shell\n\n\nTo run the spark shell, cd into the warcbase directory and run: \n\n\nspark-shell --jars target/warcbase-0.1.0-SNAPSHOT-fatjar.jar\n\n\nBy default, command in spark-shell must be one-line.\n\nTo run multi-line commands, type \n:paste\n in Spark shell to start a multi-line command, and Ctrl-D to finish the command.\n\n\nThe following script counts web pages by time:  \n\n\nimport org.warcbase.spark.rdd.RecordRDD._\nimport org.warcbase.spark.matchbox.RecordLoader\n\nval counts = RecordLoader.loadArc(\n/shared/collections/CanadianPoliticalParties/arc/\n)\n  .keepValidPages\n  .map(r =\n r.getDate)\n  .countItems()\n  .saveAsTextFile(\npath/to/output\n)\n\n\n\n\nIn the output directory you should find data output files with date and count.\n\n\nRunning with Spark Notebook\n\n\nSpark Notebook is an interactive web-based editor that can run Scala and Spark. \n\n\nTo use Spark Notebook with Warcbase, download the \nnotebook\n with Scala 2.10, Spark 1.3.0, and Hadoop 2.6.0-cdh5.4.2. \nAlso \nbuild\n Warcbase if you have not already done so.\n\n\nThen unzip the downloaded Spark Notebook file, cd into the directory, and run \n./bin/spark-notebook\n.\n\n\nThe terminal should say: \nListening for HTTP on /0:0:0:0:0:0:0:0:9000\n\n\nThen navigate to \nhttp://localhost:9000/\n in your browser.\n\n\nTo make a new notebook, click the '+' button on the top right-hand corner.\n\n\nIn that notebook, enter \n:cp /path/to/warcbase/jar\n as the first command to load Warcbase. Now you have an interactive Spark shell running Warcbase!", 
            "title": "Installing and Running Spark under OS X"
        }, 
        {
            "location": "/Warcbase-Wayback-Integration/", 
            "text": "Warcbase comes with a browser exposed as a REST API that conforms to Wayback's schema of \ncollection/YYYYMMDDHHMMSS/targetURL\n. Here's how you start the browser:\n\n\n$ setenv CLASSPATH_PREFIX \n/etc/hbase/conf/\n\n$ sh target/appassembler/bin/WarcBrowser -port 8080\n\n\n\n\nYou can now use \nhttp://myhost:8080/\n to browse the archive. For example:\n\n\n\n\nhttp://myhost:8080/mycollection/*/http://mysite.com/\n will give you a list of available versions of \nhttp://mysite.com/\n.\n\n\nhttp://myhost:8080/mycollection/19991231235959/http://mysite.com/\n will give you the record of \nhttp://mysite.com/\n just before Y2K.\n\n\n\n\nNote that this API serves up raw records, so the HTML pages don't look pretty, and images don't render properly (since the browser gets confused by record headers). So how do you actually navigate through the archive? This is where Wayback/Warcbase integration comes in.\n\n\nAs it turns out, the Wayback code has the ability separate rendering/browsing from data storage. More details can be found in this \ntechnical overview\n. In short, we can customize a Wayback instance to point at the Warcbase REST API, and have the Wayback fetch records from HBase. This is accomplished by custom implementations of \nResourceIndex\n and \nResourceStore\n in \nhere\n.\n\n\nHere's how to install the integration:\n\n\n\n\nMake sure you already have Wayback installed. See this \ninstallation guide\n and \nconfiguration guide\n.\n\n\nAdd the Warcbase jar to the Wayback's WAR deployment. In a standard setup, you would copy \nwarcbase-0.1.0-SNAPSHOT.jar\n to the \nTOMCAT_ROOT/webapps/ROOT/WEB-INF/lib/\n.\n\n\nReplace the \nBDBCollection.xml\n configuration in \nTOMCAT_ROOT/webapps/ROOT/WEB-INF/\n with the version in \nsrc/main/resources/\n.\n\n\nOpen up \nBDBCollection.xml\n and specify the correct \nHOST\n, \nPORT\n, and \nTABLE\n.\n\n\nShutdown and restart Tomcat.\n\n\n\n\nNow navigate to your Wayback as before. Enjoy browsing your web archive!", 
            "title": "Warcbase Wayback Integration"
        }, 
        {
            "location": "/Warcbase-Java-Tools/", 
            "text": "Building the URL mapping\n\n\nIt's convenient for a variety of tasks to map every URL to a unique integer id. Lucene's FST package provides a nice API for this task.\n\n\nThere are two ways to build the URL mapping, the first of which is via a MapReduce job:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.data.UrlMappingMapReduceBuilder \\\n    -input /hdfs/path/to/data -output fst.dat\n\n\n\n\nThe FST data in this case will be written to HDFS. The potential issue with this approach is that building the FST is relatively memory hungry, and cluster memory is sometimes scarce.\n\n\nThe alternative is to build the mapping locally on a machine with sufficient memory. To do this, first run a MapReduce job to extract all the unique URLs:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.ExtractUniqueUrls \\\n    -input /hdfs/path/to/data -output urls\n\n\n\n\nNow copy the \nurls/\n directory out of HDFS and then run the following program:\n\n\n$ sh target/appassembler/bin/UrlMappingBuilder -input urls -output fst.dat\n\n\n\n\nWhere \nurls\n is the output directory from above and \nfst.dat\n is the name of the FST data file. We can examine the FST data with the following utility program:\n\n\n# Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/\n\n\n\n\nNow copy the fst.dat file into HDFS for use in the next step:\n\n\n$ hadoop fs -put fst.dat /hdfs/path/\n\n\n\n\nExtracting the Webgraph\n\n\nWe can use the mapping data (from above) to extract the webgraph and at the same time map URLs to unique integer ids. This is accomplished by a Hadoop program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.graph.ExtractLinksWac \\\n    -hdfs /hdfs/path/to/data -output output -urlMapping fst.dat\n\n\n\n\nFinally, instead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, following input files are needed:\n\n\n\n\na prefix file providing URL prefixes for each supernode (comma-delimited: id, URL prefix);\n\n\nan FST mapping file to map individual URLs to unique integer ids (from above);\n\n\n\n\nThen run this MapReduce program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.graph.ExtractSiteLinks \\\n    -hdfs /hdfs/path/to/data -output output \\\n    -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv\n\n\n\n\nYou'll find site-level webgraph in \noutput/\n on HDFS.", 
            "title": "Warcbase Java Tools"
        }, 
        {
            "location": "/Warcbase-Java-Tools/#building-the-url-mapping", 
            "text": "It's convenient for a variety of tasks to map every URL to a unique integer id. Lucene's FST package provides a nice API for this task.  There are two ways to build the URL mapping, the first of which is via a MapReduce job:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.data.UrlMappingMapReduceBuilder \\\n    -input /hdfs/path/to/data -output fst.dat  The FST data in this case will be written to HDFS. The potential issue with this approach is that building the FST is relatively memory hungry, and cluster memory is sometimes scarce.  The alternative is to build the mapping locally on a machine with sufficient memory. To do this, first run a MapReduce job to extract all the unique URLs:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.ExtractUniqueUrls \\\n    -input /hdfs/path/to/data -output urls  Now copy the  urls/  directory out of HDFS and then run the following program:  $ sh target/appassembler/bin/UrlMappingBuilder -input urls -output fst.dat  Where  urls  is the output directory from above and  fst.dat  is the name of the FST data file. We can examine the FST data with the following utility program:  # Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/  Now copy the fst.dat file into HDFS for use in the next step:  $ hadoop fs -put fst.dat /hdfs/path/", 
            "title": "Building the URL mapping"
        }, 
        {
            "location": "/Warcbase-Java-Tools/#extracting-the-webgraph", 
            "text": "We can use the mapping data (from above) to extract the webgraph and at the same time map URLs to unique integer ids. This is accomplished by a Hadoop program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.graph.ExtractLinksWac \\\n    -hdfs /hdfs/path/to/data -output output -urlMapping fst.dat  Finally, instead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, following input files are needed:   a prefix file providing URL prefixes for each supernode (comma-delimited: id, URL prefix);  an FST mapping file to map individual URLs to unique integer ids (from above);   Then run this MapReduce program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar \\\n    org.warcbase.analysis.graph.ExtractSiteLinks \\\n    -hdfs /hdfs/path/to/data -output output \\\n    -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv  You'll find site-level webgraph in  output/  on HDFS.", 
            "title": "Extracting the Webgraph"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/", 
            "text": "Setting up UK Web Archive's Shine on OS X\n\n\nShawn Dickinson and Ian Milligan (Web Archives for Historical Research Group)\n\n\nTested by David Hussey\n\n\nDo you have a lot of WARCs that you want to discover? The \nUK Web Archive's Shine browser\n is a \"Prototype SOLR-powered web archive exploration UI.\" It's perfect for what historians might want to do with WARCs. This is an attempt at an easy-to-understand walkthrough to harness the power of Shine for your own collections. If you have any questions, don't hestiate to contact us via Ian at \ni2millig@uwaterloo.ca\n. \n\n\nCheck out Shine in action below:\n\n\n\n\nStep One: Setting up WebArchive-Discovery\n\n\nAdapted from https://github.com/ukwa/webarchive-discovery/wiki. \n\n\nDependencies:\n- \nBrew\n (follow the link to install brew, a hpandy package manager for OS X)\n- Java 1.7 or higher\n- Maven (\nbrew install maven\n)\n- Git (\nbrew install git\n)\n\n\nClone webarchive-discovery\n\n\nIn the directory where you want to install it, use:\n\n\ngit clone https://github.com/ukwa/webarchive-discovery.git\n\n\nRecursively cloning makes sure it includes submodules, such as bootstrap.\n\n\nBuild\n\n\ncd webarchive-discovery/warc-solr-test-server\n\n\nmvn jetty:run-exploded\n\n\nThis will initialize the solr test server. Keep it running.\n\n\nSet up the WARC Ingestor\n\n\nYou will need to download the JAR, suggested by the wiki and available \nhere\n and place it in the correct folder. We downloaded \nwarc-indexer-2.0.1-SNAPSHOT-jar-with-dependencies.jar\n. Direct link for curl/wget is: \nhttps://oss.sonatype.org/content/repositories/snapshots/uk/bl/wa/discovery/warc-indexer/2.0.1-SNAPSHOT/warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar\n\n\nFor example data:\n Run the below command, replacing SNAPSHOT with the correct snapshot id from the file that was downloaded.\n\n\njava -jar warc-indexer-2.0.1-SNAPSHOT-jar-with-dependencies.jar  -s http://localhost:8080/discovery -t warc-indexer/src/test/resources/wikipedia-mona-lisa/flashfrozen-jwat-recompressed.warc.gz\n\n\n\n\nFor own data:\n you can point it to whatever directory your warc.gz or arc.gz files are, a la:\n\n\njava -jar warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar -s http://localhost:8080/discovery -t /Users/ianmilligan1/dropbox/sample-WARCs/*.warc.gz\n\n\n\n\nThis may throw lots of errors, as ill-formed HTML causes issues - i.e. \"img\" must be terminated by the matching end-tag \"\n\". when using wild web data, this causes trouble. You can monitor progress in both this widow, as well as the previous one you have running the solr-test server. You'll see some interesting artefacts.\n\n\nIf you're ingesting a lot of ARCs/WARCs, you may want to run a quick bash script a la (you will of course need to change the directory where you have the indexer-jar):\n\n\n#!/bin/bash\n# ingest-script.sh\n# run in directory with WARCs on local node, headless prevents focus stealing (will drive you crazy)\n\nfor f in *.warc.gz # if using arcs change to *.arc.gz or if both *.gz\ndo\n    echo \nProcessing $f\n\n    java -Djava.awt.headless=true -jar /volumes/WAHR-SolrTest/webarchive-discovery/warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar -s http://localhost:8080/discovery -t $f\ndone\n\n\n\n\nStep Two: Setting up shine\n\n\nDependencies:\n- Typesafe Activator (\nbrew install typesafe-activator\n)\n- Postgres (\nbrew install postgres\n)\n\n\nFirst, clone the shine repo with the following command:\n\n\ngit clone --recursive https://github.com/ukwa/shine.git\n\n\n\n\nPlay framework\n\n\nNote that the play framework was replaced with typesafe-activator. From here on replace any \nplay\n commands that you might find in other documentation with \nactivator\n.\n\n\nInitialize shine by \ncd shine/shine\n and \nactivator run\n\n\nOn the default installation, the \"random results\" on the Trend page won't work. If you change line 180 of \n/shine/shine/app/controllers/Shiner.java\n from \nrandom_12\n to the actual field name of \nid\n, the issue will work. The new code block shoudl look like in part:\n\n\n// Grab the baseline data:\nparams.put(\nsort\n,Arrays.asList(new String[] { \nid\n } ));\nparams.put(\nfacet.in.crawl_years\n, Arrays.asList(new String[] { year } ));\nQuery sample = new Query(query,params);\n// Do the search:\nsample = solr.search(sample,100);\n\n\n\n\nPostgresql\n\n\nInitialize the Postgres server by running \npg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start\n. You can also manually stop the server at any time with \npg_ctl -D /usr/local/var/postgres stop -s -m fast\n. \n\n\nIf you want to monitor activity, the command \npostgres -D /usr/local/var/postgres\n will let you. This should run in the background (you will need to open a new terminal window as this will continue). \n\n\nWhen starting this for the first time, you will also have to use the following commands:\n\n\ncreateuser -P shine\n and enter a password for the user (this creates a db user with the name \"shine\" and your given password). Note that if you use a special character (such as !) in your password, you will need to enclose it with double quotation marks.\n\n\ncreatedb -O shine shine\n (this creates a db named \"shine\" with the user \"shine\" as its owner).\n\n\nIn \nshine/shine/conf/application.conf\n make the following changes. Comment out line 92 and uncomment line 93, so the host section looks like:\n\n\n#host = \nhttp://192.168.1.204:8983/solr/ldwa/\n,\n#host = \nhttp://192.168.45.220:8983/solr/ldwadev/\n,\n#host = \nhttp://192.168.1.151:8984/solr/jisc/\n,\n#host = \nhttp://192.168.1.181:8983/solr/jisc5\n,\nhost = \nhttp://localhost:8080/discovery/\n,\n#host = \nhttp://chrome.bl.uk:8080/solr/\n\n\n\n\n\nOn line 47, replace the default password with the password you previously created for the db user. The code block should look like:\n\n\n#db.default.logStatements=true\ndb.default.driver=org.postgresql.Driver\ndb.default.url=\njdbc:postgresql://localhost/shine\n\ndb.default.user=shine\ndb.default.password=yourpassword\n\n\n\n\nWhere yourpassword is the password you generated with the \ncreateuser\n command.\n\n\nOpening shine in the browser (http://localhost:9000), you should have a button to \"Apply this script now!\" -- click it and run the setup SQL scripts\n\n\nStep Three: Use Shine\n\n\nNavigate to \nhttp://localhost:9000/shine\n to begin using. Note that simply using http://localhost:9000/ will lead to errors. \n\n\nIf in the future, you want to re-active it, you can use this collection of easy-to-use scripts here: \nhttps://github.com/ianmilligan1/WAHR/tree/master/scripts/shine\n. Simply download the files, give them privileges with \nchmod +x shine-execute-on.sh\n and \nchmod +x solr-execute-on.sh\n and then run \nexecute.sh\n. It will launch two terminal windows for the application to run.\n\n\nNotes\n\n\nSolr Indexing Heap Space Issues\n\n\nIf the Solr index grows too large (mine is about 80GB), you may need to pass some more memory parameters to get it running on some machines. On a laptop with 16GB of memory, this command did the trick when initializing the server:\n\n\nMAVEN_OPTS='-Xmx12g -Xms1g -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4' mvn jetty:run-exploded\n\n\n\n\nThanks to @ruebot, @anjackson, @tokee.\n\n\nDefault Search Query\n\n\nBy default, the query will be \"big data\" between 1996 and 2010. You might want to change that for your collections. To do so, open: \nshine/shine/target/scala-2.10/src_managed/main/routes_routing.scala\n and change the default query on line 280. i.e. mine now reads:\n\n\n// @LINE:28\ncase controllers_Search_plot_graph10(params) =\n {\n   call(params.fromQuery[String](\nquery\n, Some(\nrecession,depression\n)), params.fromQuery[String](\nyear_start\n, Some(\n2005\n)), params.fromQuery[String](\nyear_end\n, Some(\n2015\n))) { (query, year_start, year_end) =\n\n        invokeHandler(controllers.Search.plot_graph(query, year_start, year_end), HandlerDef(this, \ncontrollers.Search\n, \nplot_graph\n, Seq(classOf[String], classOf[String], classOf[String]),\nGET\n, \n, Routes.prefix + \ngraph\n))\n   }\n}\n\n\n\n\nQuestions?\n\n\nIf you run into any questions, please let us know. You can contact Ian Milligan at \n, or fork and submit a pull request.", 
            "title": "Shine: Installing Shine Frontend on OS X"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#setting-up-uk-web-archives-shine-on-os-x", 
            "text": "Shawn Dickinson and Ian Milligan (Web Archives for Historical Research Group)  Tested by David Hussey  Do you have a lot of WARCs that you want to discover? The  UK Web Archive's Shine browser  is a \"Prototype SOLR-powered web archive exploration UI.\" It's perfect for what historians might want to do with WARCs. This is an attempt at an easy-to-understand walkthrough to harness the power of Shine for your own collections. If you have any questions, don't hestiate to contact us via Ian at  i2millig@uwaterloo.ca .   Check out Shine in action below:", 
            "title": "Setting up UK Web Archive's Shine on OS X"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#step-one-setting-up-webarchive-discovery", 
            "text": "Adapted from https://github.com/ukwa/webarchive-discovery/wiki.   Dependencies:\n-  Brew  (follow the link to install brew, a hpandy package manager for OS X)\n- Java 1.7 or higher\n- Maven ( brew install maven )\n- Git ( brew install git )  Clone webarchive-discovery  In the directory where you want to install it, use:  git clone https://github.com/ukwa/webarchive-discovery.git  Recursively cloning makes sure it includes submodules, such as bootstrap.  Build  cd webarchive-discovery/warc-solr-test-server  mvn jetty:run-exploded  This will initialize the solr test server. Keep it running.  Set up the WARC Ingestor  You will need to download the JAR, suggested by the wiki and available  here  and place it in the correct folder. We downloaded  warc-indexer-2.0.1-SNAPSHOT-jar-with-dependencies.jar . Direct link for curl/wget is:  https://oss.sonatype.org/content/repositories/snapshots/uk/bl/wa/discovery/warc-indexer/2.0.1-SNAPSHOT/warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar  For example data:  Run the below command, replacing SNAPSHOT with the correct snapshot id from the file that was downloaded.  java -jar warc-indexer-2.0.1-SNAPSHOT-jar-with-dependencies.jar  -s http://localhost:8080/discovery -t warc-indexer/src/test/resources/wikipedia-mona-lisa/flashfrozen-jwat-recompressed.warc.gz  For own data:  you can point it to whatever directory your warc.gz or arc.gz files are, a la:  java -jar warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar -s http://localhost:8080/discovery -t /Users/ianmilligan1/dropbox/sample-WARCs/*.warc.gz  This may throw lots of errors, as ill-formed HTML causes issues - i.e. \"img\" must be terminated by the matching end-tag \" \". when using wild web data, this causes trouble. You can monitor progress in both this widow, as well as the previous one you have running the solr-test server. You'll see some interesting artefacts.  If you're ingesting a lot of ARCs/WARCs, you may want to run a quick bash script a la (you will of course need to change the directory where you have the indexer-jar):  #!/bin/bash\n# ingest-script.sh\n# run in directory with WARCs on local node, headless prevents focus stealing (will drive you crazy)\n\nfor f in *.warc.gz # if using arcs change to *.arc.gz or if both *.gz\ndo\n    echo  Processing $f \n    java -Djava.awt.headless=true -jar /volumes/WAHR-SolrTest/webarchive-discovery/warc-indexer-2.0.1-20150116.110435-2-jar-with-dependencies.jar -s http://localhost:8080/discovery -t $f\ndone", 
            "title": "Step One: Setting up WebArchive-Discovery"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#step-two-setting-up-shine", 
            "text": "Dependencies:\n- Typesafe Activator ( brew install typesafe-activator )\n- Postgres ( brew install postgres )  First, clone the shine repo with the following command:  git clone --recursive https://github.com/ukwa/shine.git  Play framework  Note that the play framework was replaced with typesafe-activator. From here on replace any  play  commands that you might find in other documentation with  activator .  Initialize shine by  cd shine/shine  and  activator run  On the default installation, the \"random results\" on the Trend page won't work. If you change line 180 of  /shine/shine/app/controllers/Shiner.java  from  random_12  to the actual field name of  id , the issue will work. The new code block shoudl look like in part:  // Grab the baseline data:\nparams.put( sort ,Arrays.asList(new String[] {  id  } ));\nparams.put( facet.in.crawl_years , Arrays.asList(new String[] { year } ));\nQuery sample = new Query(query,params);\n// Do the search:\nsample = solr.search(sample,100);  Postgresql  Initialize the Postgres server by running  pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start . You can also manually stop the server at any time with  pg_ctl -D /usr/local/var/postgres stop -s -m fast .   If you want to monitor activity, the command  postgres -D /usr/local/var/postgres  will let you. This should run in the background (you will need to open a new terminal window as this will continue).   When starting this for the first time, you will also have to use the following commands:  createuser -P shine  and enter a password for the user (this creates a db user with the name \"shine\" and your given password). Note that if you use a special character (such as !) in your password, you will need to enclose it with double quotation marks.  createdb -O shine shine  (this creates a db named \"shine\" with the user \"shine\" as its owner).  In  shine/shine/conf/application.conf  make the following changes. Comment out line 92 and uncomment line 93, so the host section looks like:  #host =  http://192.168.1.204:8983/solr/ldwa/ ,\n#host =  http://192.168.45.220:8983/solr/ldwadev/ ,\n#host =  http://192.168.1.151:8984/solr/jisc/ ,\n#host =  http://192.168.1.181:8983/solr/jisc5 ,\nhost =  http://localhost:8080/discovery/ ,\n#host =  http://chrome.bl.uk:8080/solr/   On line 47, replace the default password with the password you previously created for the db user. The code block should look like:  #db.default.logStatements=true\ndb.default.driver=org.postgresql.Driver\ndb.default.url= jdbc:postgresql://localhost/shine \ndb.default.user=shine\ndb.default.password=yourpassword  Where yourpassword is the password you generated with the  createuser  command.  Opening shine in the browser (http://localhost:9000), you should have a button to \"Apply this script now!\" -- click it and run the setup SQL scripts", 
            "title": "Step Two: Setting up shine"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#step-three-use-shine", 
            "text": "Navigate to  http://localhost:9000/shine  to begin using. Note that simply using http://localhost:9000/ will lead to errors.   If in the future, you want to re-active it, you can use this collection of easy-to-use scripts here:  https://github.com/ianmilligan1/WAHR/tree/master/scripts/shine . Simply download the files, give them privileges with  chmod +x shine-execute-on.sh  and  chmod +x solr-execute-on.sh  and then run  execute.sh . It will launch two terminal windows for the application to run.", 
            "title": "Step Three: Use Shine"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#notes", 
            "text": "Solr Indexing Heap Space Issues  If the Solr index grows too large (mine is about 80GB), you may need to pass some more memory parameters to get it running on some machines. On a laptop with 16GB of memory, this command did the trick when initializing the server:  MAVEN_OPTS='-Xmx12g -Xms1g -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4' mvn jetty:run-exploded  Thanks to @ruebot, @anjackson, @tokee.  Default Search Query  By default, the query will be \"big data\" between 1996 and 2010. You might want to change that for your collections. To do so, open:  shine/shine/target/scala-2.10/src_managed/main/routes_routing.scala  and change the default query on line 280. i.e. mine now reads:  // @LINE:28\ncase controllers_Search_plot_graph10(params) =  {\n   call(params.fromQuery[String]( query , Some( recession,depression )), params.fromQuery[String]( year_start , Some( 2005 )), params.fromQuery[String]( year_end , Some( 2015 ))) { (query, year_start, year_end) = \n        invokeHandler(controllers.Search.plot_graph(query, year_start, year_end), HandlerDef(this,  controllers.Search ,  plot_graph , Seq(classOf[String], classOf[String], classOf[String]), GET ,  , Routes.prefix +  graph ))\n   }\n}", 
            "title": "Notes"
        }, 
        {
            "location": "/Shine:-Installing-Shine-Frontend-on-OS-X/#questions", 
            "text": "If you run into any questions, please let us know. You can contact Ian Milligan at  , or fork and submit a pull request.", 
            "title": "Questions?"
        }, 
        {
            "location": "/Building-Lucene-Indexes-Using-Hadoop/", 
            "text": "Warcbase integrates with the UK web archive's \nWARC indexer\n and Hadoop-related tools to build Lucene indexes using Hadoop. The command-line invocation is as follows:\n\n\nhadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.index.IndexerRunner \\\n  -input cpp-all-files.txt -index cpp-all-index -numShards 200\n\n\n\n\nThe file specified by \n-input\n is a file on local disk that lists the WARC and ARC files to be indexed. It should be a plain-text file, one (W)ARC file per line, something like:\n\n\n/collections/webarchives/CanadianPoliticalParties/arc/227-20051004191331-00000-crawling015.archive.org.arc.gz\n/collections/webarchives/CanadianPoliticalParties/arc/227-20051004192151-00001-crawling015.archive.org.arc.gz\n/collections/webarchives/CanadianPoliticalParties/arc/227-20051004192747-00002-crawling015.archive.org.arc.gz\n...\n\n\n\n\nThe option \n-index\n specifies the location on HDFS where the index will be built. The option \n-numShards\n specifies the number of shards. Indexing speed is affected by the number of shards: the more shards, the more parallelization, hence faster. But the downside is that you'll have to do shard merging later (more below). Another issue to consider is memory usage: too few shards, the shards get too big, and you'll run out of memory in your reducers.\n\n\nAfter the job completes, in `cpp-all-index/' on HDFS, you'll see 200 sub-directories, one for each shard. If you want to use Shine to search these indexes, the next thing we have to do is to merge all the shards into one unified index.\n\n\nTo merge the shards, you'll need to copy the shards out of HDFS, and then use the following Lucene tool:\n\n\njava -cp lucene-core-4.7.2.jar:lucene-misc-4.7.2.jar \\\n  org.apache.lucene.misc.IndexMergeTool [merged-index] [shard1] [shard2] ...\n\n\n\n\nYou can get the above jars from Maven central:\n\nlucene-core-4.7.2.jar\n and\n\nlucene-misc-4.7.2.jar\n\n\nYou might want to write a simple script to generate the right command. In the case of the index above, the complete merging command is:\n\n\nnohup java -cp lucene-core-4.7.2.jar:lucene-misc-4.7.2.jar org.apache.lucene.misc.IndexMergeTool cpp-index \\\n  shard1/data/index shard2/data/index shard3/data/index shard4/data/index shard5/data/index \\\n  shard6/data/index shard7/data/index shard8/data/index shard9/data/index shard10/data/index \\\n  shard11/data/index shard12/data/index shard13/data/index shard14/data/index shard15/data/index \\\n  shard16/data/index shard17/data/index shard18/data/index shard19/data/index shard20/data/index \\\n  shard21/data/index shard22/data/index shard23/data/index shard24/data/index shard25/data/index \\\n  shard26/data/index shard27/data/index shard28/data/index shard29/data/index shard30/data/index \\\n  shard31/data/index shard32/data/index shard33/data/index shard34/data/index shard35/data/index \\\n  shard36/data/index shard37/data/index shard38/data/index shard39/data/index shard40/data/index \\\n  shard41/data/index shard42/data/index shard43/data/index shard44/data/index shard45/data/index \\\n  shard46/data/index shard47/data/index shard48/data/index shard49/data/index shard50/data/index \\\n  shard51/data/index shard52/data/index shard53/data/index shard54/data/index shard55/data/index \\\n  shard56/data/index shard57/data/index shard58/data/index shard59/data/index shard60/data/index \\\n  shard61/data/index shard62/data/index shard63/data/index shard64/data/index shard65/data/index \\\n  shard66/data/index shard67/data/index shard68/data/index shard69/data/index shard70/data/index \\\n  shard71/data/index shard72/data/index shard73/data/index shard74/data/index shard75/data/index \\\n  shard76/data/index shard77/data/index shard78/data/index shard79/data/index shard80/data/index \\\n  shard81/data/index shard82/data/index shard83/data/index shard84/data/index shard85/data/index \\\n  shard86/data/index shard87/data/index shard88/data/index shard89/data/index shard90/data/index \\\n  shard91/data/index shard92/data/index shard93/data/index shard94/data/index shard95/data/index \\\n  shard96/data/index shard97/data/index shard98/data/index shard99/data/index shard100/data/index \\\n  shard101/data/index shard102/data/index shard103/data/index shard104/data/index shard105/data/index \\\n  shard106/data/index shard107/data/index shard108/data/index shard109/data/index shard110/data/index \\\n  shard111/data/index shard112/data/index shard113/data/index shard114/data/index shard115/data/index \\\n  shard116/data/index shard117/data/index shard118/data/index shard119/data/index shard120/data/index \\\n  shard121/data/index shard122/data/index shard123/data/index shard124/data/index shard125/data/index \\\n  shard126/data/index shard127/data/index shard128/data/index shard129/data/index shard130/data/index \\\n  shard131/data/index shard132/data/index shard133/data/index shard134/data/index shard135/data/index \\\n  shard136/data/index shard137/data/index shard138/data/index shard139/data/index shard140/data/index \\\n  shard141/data/index shard142/data/index shard143/data/index shard144/data/index shard145/data/index \\\n  shard146/data/index shard147/data/index shard148/data/index shard149/data/index shard150/data/index \\\n  shard151/data/index shard152/data/index shard153/data/index shard154/data/index shard155/data/index \\\n  shard156/data/index shard157/data/index shard158/data/index shard159/data/index shard160/data/index \\\n  shard161/data/index shard162/data/index shard163/data/index shard164/data/index shard165/data/index \\\n  shard166/data/index shard167/data/index shard168/data/index shard169/data/index shard170/data/index \\\n  shard171/data/index shard172/data/index shard173/data/index shard174/data/index shard175/data/index \\\n  shard176/data/index shard177/data/index shard178/data/index shard179/data/index shard180/data/index \\\n  shard181/data/index shard182/data/index shard183/data/index shard184/data/index shard185/data/index \\\n  shard186/data/index shard187/data/index shard188/data/index shard189/data/index shard190/data/index \\\n  shard191/data/index shard192/data/index shard193/data/index shard194/data/index shard195/data/index \\\n  shard196/data/index shard197/data/index shard198/data/index shard199/data/index shard200/data/index \n log.txt", 
            "title": "Building Lucene Indexes Using Hadoop"
        }, 
        {
            "location": "/Building-and-Running-Warcbase-Under-OS-X/", 
            "text": "Warcbase is a web archive \nplatform\n, not a single program. Its capabilities comprise two main categories:\n\n\n\n\nAnalysis of web archives using the \nPig\n programming language, and assorted helper scripts and utilities\n\n\nWeb archive database management, with support for the \nHBase\n distributed data store, and \nOpenWayback\n integration providing a friendly web interface to view stored websites\n\n\n\n\nOne can take advantage of the analysis tools (1) without bothering with the database management aspect of Warcbase -- in fact, most digital humanities researchers will probably find the former more useful. Users who are only interested in the analysis tools need only be concerned with the first two sections of this document (Prerequisites and Building Warcbase).\n\n\n(This document was written because installing Warcbase under OS X requires a number of minor changes to the \nofficial project instructions\n.)\n\n\nPrerequisites\n\n\n\n\nOS X Developer Tools\n\n\nHomebrew\n\n\nMaven (\nbrew install maven\n)\n\n\nHadoop (\nbrew install hadoop\n)\n\n\n\n\nHBase (\nbrew install hbase\n)\n\n\nConfigure HBase by making the changes to the following files located in the HBase installation directory, which will be something like \n/usr/local/Cellar/hbase/0.98.6.1/libexec/\n (depending on the version number).\n\n\n\n\nconf/hbase-site.xml\n:\nInsert within the \n tags\n\n\n    \nhbase.rootdir\n\n    \nfile:///Users/yourname/hbase\n\n\n\n\n\n    \nhbase.zookeeper.property.dataDir\n\n    \n/Users/yourname/zookeeper\n\n\n\n\n\n\nWhere \nyourname\n is your username. Feel free to choose other directories to store these files, used by HBase and its ZooKeeper instance, if you like. \nHBase will create these directories. If they already exist, they will cause problems later on.\n\n\n\n\nconf/hbase-env.sh\n: Look for the following line,\n      export HBASE_OPTS=\"-XX:+UseConcMarkSweepGC\"\n\n\n\n\nand change it to:\n\n\n  export HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -Djava.security.krb5.realm=-Djava.security.krb5.kdc=\"\n\n\n\nVerify that HBase is installed correctly by running the HBase shell:\n\n\n$ hbase shell\n\nhbase(main):001:0\n list\n\n// Some lines of log messages\n\n0 row(s) in 1.3060 seconds\n\n=\n []\n\nhbase(main):002:0\n exit\n\n\n\n\n\n\n\nTomcat (\nbrew install tomcat\n) \n(Only necessary for OpenWayback integration; skip otherwise)\n \n\n\n\n\n\n\nPig (\nbrew install pig\n)\n\n\n\n\n\n\n\n\nN.B.\n If you run an automatic Homebrew system update (\nbrew update \n brew upgrade\n) it is possible a new version of Hadoop, HBase, or Tomcat will be installed. The previous version will remain on your system, but the symbolic links in \n/ur/local/bin/\n will point to the new version; i.e., it is the new version that will be executed when you run any of the software's components, unless you specify the full pathname. There are two solutions:\n1. Re-configure the new version of the updated software according to the instructions above.\n2. Make the symbolic links point to the older version of the updated software, with the command \nbrew switch \nformula\n \nversion\n. E.g., \nbrew switch hbase 0.98.6.1\n.\n\n\n\n\nBuilding Warcbase\n\n\nTo start, you will need to clone the Warcbase Git repository:\n\n\n$ git clone http://github.com/lintool/warcbase.git\n\n\n\nFrom inside the root directory \nwarcbase\n, build the project:\n\n\n$ mvn clean package appassembler:assemble -DskipTests\n\n\n\nIf you leave off \n-DskipTests\n, the build may fail when it runs tests due to a shortage of memory. If you try the build with the tests and this happens, don't worry about it. \n\n\nBecause OS X is not quite case sensitive (it does not allow two files or directories spelled the same but for case), you must remove one file from the JAR package:\n\n\n$ zip -d target/warcbase-0.1.0-SNAPSHOT-fatjar.jar META-INF/LICENSE\n\n\n\nIngesting content\n\n\nTo ingest a directory of web archive files (which may be GZip-compressed, e.g., webcollection.arc.gz), run the following from inside the \nwarcbase\n directory. \n\n\n$ start-hbase.sh\n$ export CLASSPATH_PREFIX=\"/usr/local/Cellar/hbase/0.98.6.1/libexec/conf/\"\n$ sh target/appassembler/bin/IngestFiles -dir /path/to/webarchive/files/ -name archive_name -create -gz\n\n\n\nChange as appropriate the HBase configuration path (version number), the directory of web archive files, and the archive name. Use the option \n-append\n instead of \n-create\n to add to an existing database table. Note the \n-gz\n flag: this changes compression method to Gzip from the default Snappy, which is unavailable as a native Hadoop library on OS X. (The above commands assume you are using a shell in the \nbash\n family.)\n\n\nTip\n: To avoid repeatedly setting the CLASSPATH_PREFIX variable, add the \nexport\n line to your \n~/.bash_profile\n file.\n\n\nIf you wish to shut down HBase, the command is \nstop-hbase.sh\n. You can check if HBase is running with the command \njps\n; if it is running you will see the process \nHMaster\n listed. You can also view detailed server status information at http://localhost:60010/.\n\n\nRun and test the WarcBrowser\n\n\nYou may now view your archived websites through the WarcBrowser interface.\n\n\n# Start HBase first, if it isn't already running:\n$ start-hbase.sh\n# Set CLASSPATH_PREFIX, if it hasn't been done this terminal session:\n$ export CLASSPATH_PREFIX=\"/usr/local/Cellar/hbase/0.98.6.1/libexec/conf/\"\n# Start the browser:\n$ sh target/appassembler/bin/WarcBrowser -port 8079\n\n\n\nYou can now use \nhttp://localhost:8079/\n to browse the archive. For example:\n\n \nhttp://localhost:8079/archive_name/*/http://mysite.com/\n will give you a list of available versions of \nhttp://mysite.com/\n.\n\n \nhttp://localhost:8079/archive_name/19991231235959/http://mysite.com/\n will give you the record of \nhttp://mysite.com/\n just before Y2K.\n\n\nOpenWayback Integration\n\n\nFor a more functional, visually-appealing interface, you may install a custom version of the Wayback Machine. \n\n\nAssuming Tomcat is installed, start it by running:\n\n\n$ catalina start\n\n\n\nInstall OpenWayback by downloading the latest binary release \nhere\n. Extract the .tar.gz; inside it there will be a web application file \nopenwayback-(version).war\n. Copy this file into the \nwebapps\n folder of Tomcat, something like \n/usr/local/Cellar/tomcat/8.0.17/libexec/webapps/\n, and rename it \nROOT.war\n. Tomcat will immediately unpack this file into the \nROOT\n directory.\n\n\nIf you are running a current version of Tomcat (i.e., version 8) in combination with OpenWayback 2.0.0, edit the file \nwebapps/ROOT/WEB-INF/web.xml\n and insert a slash (\"/\") in front of the paths of parameters \nlogging-config-path\n and \nconfig-path\n. (\nDetails\n) Future releases of OpenWayback should already include this configuration change.\n\n\nAdd the Warcbase jar file to the Wayback installation, by copying \ntarget/appassembler/repo/org/warcbase/warcbase/0.1.0-SNAPSHOT/warcbase-0.1.0-SNAPSHOT.jar\n from the Warcbase build directory into Tomcat's \nwebapps/ROOT/WEB-INF/lib/\n. \n\n\nInto \nwebapps/ROOT/WEB-INF\n copy Warcbase's \nsrc/main/resources/BDBCollection.xml\n. In \nBDBCollection.xml\n, replace \nHOST\n, \nPORT\n, and \nTABLE\n with \nlocalhost\n, \n8079\n, and \narchive_name\n (or whatever the archive table in HBase is called).\n\n\nRestart Tomcat:\n\n\n$ catalina stop\n$ catalina start\n\n\n\nNow, navigate to http://localhost:8080/wayback/ and access one of your archived web pages through the Wayback interface.\n\n\nAnalytics\n\n\nWarcbase is useful for managing web archives, but its real power is as a platform for processing and analyzing the archives in its database. Its analysis tools are still under development, but at the moment you can use the tools described below, including the \npig\n scripting interface.\n\n\nBuilding the URL mapping\n\n\nMost of the tools that follow require a URL mapping file, which maps every URL in a set of ARC/WARC files to a unique integer ID. There are two ways of doing this; the first is simpler:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.UrlMappingMapReduceBuilder -input /path/to/webarc/files -output fst.dat\n\n\n\nIf this does not work due to a lack of memory, try the following steps:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /path/to/webarchive/files -output urls\n\n\n\n(If you have configured HBase to run in distributed mode rather than in standalone mode, which is the configuration provided above, you must now copy the \nurls\n directory out of HDFS into the local filesystem.)\n\n\nNext:\n\n\n$ sh target/appassembler/bin/UrlMappingBuilder -input /path/to/urls -output fst.dat\n\n\n\nWe can examine the FST data with the following utility program:\n\n\n# Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/\n\n\n\n(If you are running in distributed mode, now copy the \nfst.dat\n file into the HDFS, so it is accessible to the cluster:\n\n\n$ hadoop fs -put fst.dat /hdfs/path/\n\n\n\n)\n\n\n\n\nYou might have noticed, we are working here with ARC/WARC files and not tables in HBase. The same is done below as well. This is because most of the tools described her and below do not yet have HBase support.\n\n\n\n\nExtracting the webgraph\n\n\nWe can use the mapping data from above to extract the webgraph, with a Hadoop program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.wa\nrcbase.analysis.graph.ExtractLinksWac -hdfs /path/to/webarchive/files -output output -urlMapping fst.dat\n\n\n\nThe \n-hdfs\n flag is misleading; if HBase is running in standalone mode, this flag should specifiy a local path. \n\n\nThe resulting webgraph will appear in the \noutput\n directory, in one or more files with names like \npart-m-00000\n, \npart-m-00001\n, etc.\n\n\nExtracting a site-level webgraph\n\n\nInstead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, the following input files are needed:\n\n a CSV prefix file providing URL prefixes for each supernode (comma-delimited: ID, URL prefix). The first line of this file is ignored (reserved for headers). The URL prefix is a simple string representing a site, e.g., \nhttp://cnn.com/\n. The ID should be unique (I think), so take not of the total number of unique URLs extracted when building the URL mapping above, and make sure your IDs are larger than this.\n\n an FST mapping file to map individual URLs to unique integer ids (from above)\n\n\nThen run this MapReduce program:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.ExtractSiteLinks -hdfs /path/to/webarchive/files -output output -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv\n\n\n\nOther analysis tools\n\n\nThe tools described in this section are relatively simple. Some can process ARC and/or WARC files, while others exist in ARC and WARC versions.\n\n\nThere are three counting tools. They count different content-types, crawl-dates, and urls:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcContentTypes -input /arc/files/ -output contentTypes\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcCrawlDates -input /arc/files/ -output crawlDates\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcUrls -input ~/arc/files/ -output urls\n\n\n\nFor WARC files, replace \"Arc\" with \"Warc\" in the class names (e.g., \norg.warcbase.analysis.CountArcContentTypes\n becomes \norg.warcbase.analysis.CountWarcContentTypes\n).\n\n\nThere is a tool to extract unique URLs:\n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /arc/or/warc/files -output uniqueUrls\n\n\n\nThere is a pair of tools to find URLs, according to a regex pattern. \n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindArcUrls -input /arc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindWarcUrls -input /warc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n\n\nThere is a tool to detect duplicates in the HBase:\n\n\n$ sh target/appassembler/bin/DetectDuplicates -name table\n\n\n\nThere is also a web graph tool that pulls out link anchor text (background: https://github.com/lintool/warcbase/issues/8). \n\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.graph.InvertAnchorText -hdfs /arc/or/warc/files -output output -numReducers 1 -urlMapping fst.dat\n\n\n\nFor all of these tools that employ a URL mapping, you must use the FST mapping generated for the set of data files you are analyzing.\n\n\nPig integration\n\n\nWarcbase comes with Pig integration for manipulating web archive data. Pig scripts may be run from the interactive Grunt shell (run \npig\n), or, more conveniently, from a file (e.g., \npig -f extractlinks.pig\n).\n\n\nThe following script extracts links:\n\n\nregister 'target/warcbase-0.1.0-SNAPSHOT-fatjar.jar';\n\nDEFINE ArcLoader org.warcbase.pig.ArcLoader();\nDEFINE ExtractLinks org.warcbase.pig.piggybank.ExtractLinks();\n\nraw = load '/path/to/arc/files' using ArcLoader as\n  (url: chararray, date: chararray, mime: chararray, content: bytearray);\n\na = filter raw by mime == 'text/html';\nb = foreach a generate url, FLATTEN(ExtractLinks((chararray) content));\n\nstore b into '/output/path/';\n\n\n\n\nIn the output directory you should find data output files with source URL, target URL, and anchor text.", 
            "title": "Building and Running Warcbase Under OS X"
        }, 
        {
            "location": "/Building-and-Running-Warcbase-Under-OS-X/#analytics", 
            "text": "Warcbase is useful for managing web archives, but its real power is as a platform for processing and analyzing the archives in its database. Its analysis tools are still under development, but at the moment you can use the tools described below, including the  pig  scripting interface.  Building the URL mapping  Most of the tools that follow require a URL mapping file, which maps every URL in a set of ARC/WARC files to a unique integer ID. There are two ways of doing this; the first is simpler:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.UrlMappingMapReduceBuilder -input /path/to/webarc/files -output fst.dat  If this does not work due to a lack of memory, try the following steps:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /path/to/webarchive/files -output urls  (If you have configured HBase to run in distributed mode rather than in standalone mode, which is the configuration provided above, you must now copy the  urls  directory out of HDFS into the local filesystem.)  Next:  $ sh target/appassembler/bin/UrlMappingBuilder -input /path/to/urls -output fst.dat  We can examine the FST data with the following utility program:  # Lookup by URL, fetches the integer id\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getId http://www.foo.com/\n# Lookup by id, fetches the URL\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getUrl 42\n# Fetches all URLs with the prefix\n$ sh target/appassembler/bin/UrlMapping -data fst.dat -getPrefix http://www.foo.com/  (If you are running in distributed mode, now copy the  fst.dat  file into the HDFS, so it is accessible to the cluster:  $ hadoop fs -put fst.dat /hdfs/path/  )   You might have noticed, we are working here with ARC/WARC files and not tables in HBase. The same is done below as well. This is because most of the tools described her and below do not yet have HBase support.   Extracting the webgraph  We can use the mapping data from above to extract the webgraph, with a Hadoop program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.wa\nrcbase.analysis.graph.ExtractLinksWac -hdfs /path/to/webarchive/files -output output -urlMapping fst.dat  The  -hdfs  flag is misleading; if HBase is running in standalone mode, this flag should specifiy a local path.   The resulting webgraph will appear in the  output  directory, in one or more files with names like  part-m-00000 ,  part-m-00001 , etc.  Extracting a site-level webgraph  Instead of extracting links between individual URLs, we can extract the site-level webgraph by aggregating all URLs with common prefix into a \"supernode\". Link counts between supernodes represent the total number of links between individual URLs. In order to do this, the following input files are needed:  a CSV prefix file providing URL prefixes for each supernode (comma-delimited: ID, URL prefix). The first line of this file is ignored (reserved for headers). The URL prefix is a simple string representing a site, e.g.,  http://cnn.com/ . The ID should be unique (I think), so take not of the total number of unique URLs extracted when building the URL mapping above, and make sure your IDs are larger than this.  an FST mapping file to map individual URLs to unique integer ids (from above)  Then run this MapReduce program:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.data.ExtractSiteLinks -hdfs /path/to/webarchive/files -output output -numReducers 1 -urlMapping fst.dat -prefixFile prefix.csv  Other analysis tools  The tools described in this section are relatively simple. Some can process ARC and/or WARC files, while others exist in ARC and WARC versions.  There are three counting tools. They count different content-types, crawl-dates, and urls:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcContentTypes -input /arc/files/ -output contentTypes\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcCrawlDates -input /arc/files/ -output crawlDates\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.CountArcUrls -input ~/arc/files/ -output urls  For WARC files, replace \"Arc\" with \"Warc\" in the class names (e.g.,  org.warcbase.analysis.CountArcContentTypes  becomes  org.warcbase.analysis.CountWarcContentTypes ).  There is a tool to extract unique URLs:  $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar org.warcbase.analysis.ExtractUniqueUrls -input /arc/or/warc/files -output uniqueUrls  There is a pair of tools to find URLs, according to a regex pattern.   $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindArcUrls -input /arc/files/ -output foundUrls -pattern \"http://.*org/.*\"\n\n$ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.FindWarcUrls -input /warc/files/ -output foundUrls -pattern \"http://.*org/.*\"  There is a tool to detect duplicates in the HBase:  $ sh target/appassembler/bin/DetectDuplicates -name table  There is also a web graph tool that pulls out link anchor text (background: https://github.com/lintool/warcbase/issues/8).   $ hadoop jar target/warcbase-0.1.0-SNAPSHOT-fatjar.jar  org.warcbase.analysis.graph.InvertAnchorText -hdfs /arc/or/warc/files -output output -numReducers 1 -urlMapping fst.dat  For all of these tools that employ a URL mapping, you must use the FST mapping generated for the set of data files you are analyzing.  Pig integration  Warcbase comes with Pig integration for manipulating web archive data. Pig scripts may be run from the interactive Grunt shell (run  pig ), or, more conveniently, from a file (e.g.,  pig -f extractlinks.pig ).  The following script extracts links:  register 'target/warcbase-0.1.0-SNAPSHOT-fatjar.jar';\n\nDEFINE ArcLoader org.warcbase.pig.ArcLoader();\nDEFINE ExtractLinks org.warcbase.pig.piggybank.ExtractLinks();\n\nraw = load '/path/to/arc/files' using ArcLoader as\n  (url: chararray, date: chararray, mime: chararray, content: bytearray);\n\na = filter raw by mime == 'text/html';\nb = foreach a generate url, FLATTEN(ExtractLinks((chararray) content));\n\nstore b into '/output/path/';  In the output directory you should find data output files with source URL, target URL, and anchor text.", 
            "title": "Analytics"
        }, 
        {
            "location": "/Gephi:-Converting-Site-Link-Structure-into-Dynamic-Visualization/", 
            "text": "One popular digital humanities tool is \nGephi\n. You can quickly use it to convert output into a dynamic, date-ordered visualization to see how link structures have changed over time.\n\n\nStep One: Convert Site Link Structure into GDF Format\n\n\n\nUse \npig2gdf.py\n. You can download it from \nhttps://github.com/ianmilligan1/WAHR/tree/master/code\n. Copy it into the directory that you want to use it in, or call it remotely. Usage is as follows:\n\n\npig2gdf.py usage:\n$ ./pig2gdf.py \nfile\n \n \nOUTPUT\n\n\n\n\n\nExample usage:\n\n\n./pig2gdf.py part-m-00000 \n political-links.gdf\n\n\n\n\nStep Two: Import into Gephi\n\n\n\nYou now want to take it into Gephi. Start Gephi (\nif you have trouble running it, this tutorial might help\n). Open the GDF file that you just generated. Click OK.\n\n\nNow visit the 'Data Laboratory' panel and do the following. Select the 'edges' table so it looks like this.\n\n\n\n\nClick on the 'Merge Columns' button and do this:\n\n\n\n\nMake sure to parse dates as \nyyyymm\n.\n\n\nThe final step is to click on 'nodes' in the upper left, click 'copy data to other column,' select 'id,' and copy to 'label.'\n\n\n\n\nStep Three: Explore your Data\n\n\n\nYou'll now notice that you have the option to enable a dynamic timeslider at the bottom of your Gephi window. \n\n\nWhile a full tutorial in Gephi visualization is outside the scope of this document, you may want to use the following to expand the link nodes. Select the 'overview' tab to see a preview of the node structure. Under layout, use the 'Force Atlas' visualization to move the nodes.", 
            "title": "Gephi: Converting Site Link Structure into Dynamic Visualization"
        }
    ]
}